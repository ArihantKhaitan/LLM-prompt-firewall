# ğŸ”’ LLM Prompt Firewall

A lightweight and extensible firewall framework to **detect and classify** LLM prompts as either **safe** or **jailbreak** using models like Ollama's "Llama3".
Designed for evaluating and filtering potentially malicious user input before it reaches your language model.

## ğŸš€ Features

- âœ… **Prompt Classification**: Classifies prompts into `safe` or `jailbreak`
- ğŸ§  **Zero-shot Inference**: Uses powerful LLMs to detect prompt intent without fine-tuning
- ğŸ”„ **Dual Model Support**: Works with both **OpenAI** and **Ollama** models
- ğŸ§ª **Evaluation Suite**: Test and measure accuracy on labeled datasets
- ğŸŒ **UI Interface**: Minimal terminal-based interface for testing
- ğŸ” **Environment Variable Based Key Loading** (no secrets stored)

## ğŸ—‚ File Structure

ğŸ“ LLM-prompt-firewall
â”‚
â”œâ”€â”€ config.py # Configuration and constants
â”œâ”€â”€ prompts.json # Input prompts to be tested
â”œâ”€â”€ dataset.json # Dataset for evaluation
â”‚
â”œâ”€â”€ classifier.py # Main Ollama-based classifier
â”œâ”€â”€ zero_shot_classifier_ollama.py # Ollama-based classifier
â”‚
â”œâ”€â”€ evaluate_classifier.py # Evaluates classifier performance
â”œâ”€â”€ run_combined_test.py # Runs tests across both models
â”œâ”€â”€ run_full_evaluation.py # Full-scale evaluation of all prompts
â”œâ”€â”€ test_firewall.py # Unit tests for firewall system
â”‚
â”œâ”€â”€ firewall.py # Core firewall logic
â”œâ”€â”€ firewall_ui.py # CLI-based UI
â”‚
â”œâ”€â”€ results/ # ğŸ“ (you can create this folder)
â”‚ â”œâ”€â”€ classified_results.json # Results from OpenAI classifier
â”‚ â”œâ”€â”€ classified_results_ollama.json # Results from Ollama classifier
â”‚ â””â”€â”€ full_firewall_eval_results.json # Final firewall output
â”‚
â””â”€â”€ README.md

## âš™ï¸ Setup Instructions

1.Clone the repo:
git clone https://github.com/ArihantKhaitan/LLM-prompt-firewall.git
cd LLM-prompt-firewall
   
2.Create a virtual environment:
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

3.Install dependencies:
pip install -r requirements.txt

4.Set your API keys:
For Hugging Face:
export HUGGINGFACE_API_KEY="your-hf-token"

ğŸ§ª How to Run

â¤ Classify Prompts (Ollama):
python zero_shot_classifier_ollama.py

â¤ Evaluate Classifier:
python evaluate_classifier.py

â¤ Run Full Evaluation:
python run_full_evaluation.py

â¤ Run Firewall UI:
streamlit run firewall_ui.py

ğŸ’¾ Output & Results

All results are saved in JSON format in the results/ directory:

classified_results_ollama.json: Ollama classification

full_firewall_eval_results.json: Final unified evaluation results

ğŸ“œ License

This project is licensed under the MIT License. You are free to use, modify, and distribute this code with proper attribution.

ğŸ‘¤ Author

Arihant Khaitan

ğŸ¤ Contributions

Pull requests and feedback are welcome. If you find a bug or have a suggestion, feel free to open an issue.
