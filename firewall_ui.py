import streamlit as st
from classifier import classify_prompt
from firewall import call_main_llm
from config import USE_FEW_SHOT
import json

# ğŸ› ï¸ Setup
st.set_page_config(page_title="LLM Prompt Firewall", layout="wide")

# ğŸŒ— Theme
mode = st.sidebar.radio("ğŸ¨ Theme", ["Light", "Dark"])
if mode == "Dark":
    st.markdown("""
        <style>
        body, .stApp {
            background-color: #121212;
            color: white;
        }
        textarea, input, .stButton>button {
            background-color: #1e1e1e;
            color: white;
        }
        </style>
    """, unsafe_allow_html=True)

# ğŸ“˜ Title & Description
st.title("ğŸ›¡ï¸ LLM Prompt Firewall")
st.caption("Test prompts using classification + LLM response safety checks")

st.markdown("---")

# ğŸ“ Input Prompt
prompt = st.text_area("ğŸ”¤ Enter a prompt to test:", height=150)

# âš™ï¸ Options
col1, col2 = st.columns(2)
with col1:
    firewall_enabled = st.toggle("ğŸ›¡ï¸ Enable Firewall", value=True)
with col2:
    few_shot_enabled = st.toggle("ğŸ§  Use Few-Shot Classification", value=USE_FEW_SHOT)

# ğŸš€ Trigger Evaluation
if st.button("Run Full Evaluation"):
    st.markdown("### ğŸ§ª Evaluation Output")

    # Store results here (matches JSON structure)
    result = {
        "prompt": prompt,
        "response": None,
        "predicted": None,
        "true_label": "unknown",  # You can replace with actual if used
        "blocked_at": None
    }

    # ğŸ” Step 1: Prompt classification
    if firewall_enabled:
        prompt_class = classify_prompt(prompt, few_shot=few_shot_enabled)
        st.write(f"**Prompt Classification:** `{prompt_class}`")

        if prompt_class in ["jailbreak", "unknown"]:
            st.error(f"âŒ Blocked at prompt stage. Classification: `{prompt_class}`")
            result["predicted"] = prompt_class
            result["blocked_at"] = "prompt"

    # ğŸ¤– Step 2: LLM call
    if result["blocked_at"] is None:
        with st.spinner("Calling LLM..."):
            response = call_main_llm(prompt)

        result["response"] = response
        st.text_area("ğŸ’¬ LLM Response", response, height=200)

        # ğŸ” Step 3: Response classification
        if firewall_enabled:
            response_class = classify_prompt(response, few_shot=few_shot_enabled)
            st.write(f"**Response Classification:** `{response_class}`")

            if response_class in ["jailbreak", "unknown"]:
                st.warning("âš ï¸ Blocked at response stage.")
                result["predicted"] = response_class
                result["blocked_at"] = "response"
            else:
                st.success("âœ… Response is safe.")
                result["predicted"] = "benign"
        else:
            st.success("âœ… Firewall disabled. Response shown directly.")
            result["predicted"] = "benign"
    else:
        result["response"] = None

    # âœ… Final result output
    st.markdown("### ğŸ“„ Result Summary")
    st.json(result)

    # Save locally
    with open("single_eval_result.json", "w") as f:
        json.dump(result, f, indent=2)

# ğŸ“‚ JSON Upload Viewer
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ“ View Evaluation JSON")
uploaded_file = st.sidebar.file_uploader("Upload `.json` results", type="json")

if uploaded_file:
    data = json.load(uploaded_file)
    st.sidebar.write(f"Total Entries: {len(data)}" if isinstance(data, list) else "1 Entry")

    # Show breakdown
    if st.sidebar.checkbox("ğŸ“Š Show Breakdown"):
        if isinstance(data, dict):
            data = [data]
        blocked_prompt = sum(1 for r in data if r["blocked_at"] == "prompt")
        blocked_response = sum(1 for r in data if r["blocked_at"] == "response")
        clean = len(data) - blocked_prompt - blocked_response

        st.sidebar.metric("ğŸ›‘ Blocked at Prompt", blocked_prompt)
        st.sidebar.metric("âš ï¸ Blocked at Response", blocked_response)
        st.sidebar.metric("âœ… Clean Completions", clean)
